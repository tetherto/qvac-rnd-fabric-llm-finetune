# qvac-finetune Release Binaries

Pre-built binaries for all supported platforms - ready to use out of the box.


### ğŸ“¦ What's Included

Each platform binary includes pre-built executables and libraries:
- âœ… `llama-finetune` - Full fine-tuning binary
- âœ… `llama-finetune-lora` - LoRA fine-tuning binary
- âœ… `llama-cli` - Inference and interactive chat
- âœ… `llama-quantize` - Model quantization tool
- âœ… `llama-perplexity` - Model evaluation tool
- âœ… `llama-export-lora` - Export/merge LoRA adapters
- âœ… All required shared libraries (GGML, backend libraries)

**Datasets & Examples:**
- ğŸ§ª [Evaluation](../evaluation/) - Scripts, reports, datasets and examples
- ğŸ“– [Documentation](../docs/) - Comprehensive benchmarks

---

## ğŸš€ Quick Download

| Platform | GPU/Hardware | Backend | Size | Download |
|----------|--------------|---------|------|----------|
| **Android** | Qualcomm Adreno, ARM Mali | Vulkan | 180MB | [ğŸ“¥ Download](./android/qvac-android-adreno-arm64-v1.0.zip) |
| **macOS** | Apple Silicon (M1-M4) | Metal | 35MB | [ğŸ“¥ Download](./macos/qvac-macos-apple-silicon-v1.0.zip) |
| **macOS** | Intel x64 | CPU | 36MB | [ğŸ“¥ Download](./macos/qvac-macos-intel-v1.0.zip) |
| **iOS** | Apple A-series | Metal | 1.3MB | [ğŸ“¥ Download](./ios/qvac-ios-v1.0.zip) |
| **Linux** | AMD/Intel/NVIDIA | Vulkan | 55MB | [ğŸ“¥ Download](./linux/qvac-linux-vulkan-x64-v1.0.zip) |
| **Linux** | ARM64 | CPU | 37MB | [ğŸ“¥ Download](./linux/qvac-linux-arm64-v1.0.zip) |
| **Linux** | Intel GPU | SYCL | 56MB | [ğŸ“¥ Download](./linux/qvac-linux-sycl-intel-v1.0.zip) |

---

## ğŸ“š Platform-Specific Guides

Detailed installation and usage instructions for each platform:

- **[Android Guide](./android/README.md)** - Setup for Adreno and Mali GPUs
- **[macOS Guide](./macos/README.md)** - Apple Silicon and Intel Macs
- **[iOS Guide](./ios/README.md)** - iPhone and iPad setup
- **[Linux Guide](./linux/README.md)** - Vulkan, SYCL, and ARM64

---

## âœ… Platform Support Matrix

| Platform | Inference | Full Fine-tuning | LoRA Fine-tuning | Instruction Tuning | Checkpointing |
|----------|-----------|------------------|------------------|-------------------|---------------|
| Android (Adreno) | âœ… | âœ… | âœ… | âœ… | âœ… |
| Android (Mali) | âœ… | âœ… | âœ… | âœ… | âœ… |
| macOS (M-series) | âœ… | âœ… | âœ… | âœ… | âœ… |
| macOS (Intel) | âœ… | âœ… | âœ… | âœ… | âœ… |
| iOS (A-series) | âœ… | âœ… | âœ… | âœ… | âœ… |
| Linux (Vulkan) | âœ… | âœ… | âœ… | âœ… | âœ… |
| Linux (SYCL) | âœ… | âœ… | âœ… | âœ… | âœ… |
| Linux (ARM64) | âœ… | âœ… | âœ… | âœ… | âœ… |

---

## ğŸ¯ Verified Model Support

All platforms have been tested with:

| Model Family | Sizes | Quantizations | Status |
|--------------|-------|---------------|--------|
| **Qwen3** | 0.6B, 1.7B, 4B | F32, F16, Q8_0, Q4_0 | âœ… Fully Supported |
| **Gemma-3** | 1B, 4B | F32, F16, Q8_0, Q4_0 | âœ… Fully Supported |
| **LLaMA 3.2** | 1B, 3B | F32, F16, Q8_0, Q4_0 | âœ… Fully Supported |
| **TinyLlama** | 1.1B | F32, F16, Q8_0, Q4_0 | âœ… Fully Supported |

---

## ğŸ“Š Performance Overview

### Inference Speed (tokens/second, Qwen3-1.7B Q8_0)

| Hardware | Peak | Average | TTFT |
|----------|------|---------|------|
| RTX 4090 | 180+ | 176 | 7ms |
| AMD 7900 XTX | 180 | 158 | 10ms |
| Apple M3 Pro | 90 | 62-90 | 37ms |
| Intel Arc A770 | 113 | 90 | 78ms |
| Adreno 830 | 35 | 17-24 | 609ms |
| Mali G715 | 8.3 | 7.8 | 795ms |

### Training Speed (tokens/second, Qwen3-1.7B Q8_0, LoRA)

| Hardware | Training t/s | Time/Epoch | Full Training (8 epochs) |
|----------|-------------|------------|--------------------------|
| RTX 4090 | 116 | 5.5 min | 45 min |
| AMD 7900 XTX | 47 | 13 min | 1.7 hrs |
| Apple M3 Pro | 17.5 | 40 min | 5.3 hrs |
| Intel Arc A770 | 30 | 20 min | 2.7 hrs |
| Adreno 830 | 6 | 1h 40min | 13 hrs |
| Mali G715 | 1.3 | 7h 40min | 61 hrs |

> ğŸ“Š See [complete benchmarks](../docs/BENCHMARKS.md) for detailed metrics

---

## ğŸ”¬ Quality Validation

### vs PyTorch/HuggingFace

| Metric | qvac-finetune | PyTorch |
|--------|---------------|---------|
| **LLM-as-Judge Win Rate** | 45-48% | 52-55% |
| **Biomedical Accuracy** | 79-94% | 78-86% |
| **Cosine Similarity** | 0.82 | 0.77 |
| **Jaccard Similarity** | 0.19 | 0.23 |

**Conclusion:** Near-parity with established frameworks, proving cross-platform training maintains quality.

---

## ğŸ†• What's New in v1.0

### Core Features
- âœ… **Cross-Platform LoRA Training** - Works on all modern GPUs
- âœ… **Instruction Fine-Tuning** - Masked-loss training for alignment
- âœ… **Dynamic Tiling** - Solves Adreno GPU memory constraints
- âœ… **Checkpointing System** - Save and resume training
- âœ… **Learning Rate Scheduling** - Cosine, linear, and constant schedulers

### Architecture Support
- âœ… **GEGLU Backward Pass** - Enables Gemma fine-tuning
- âœ… **OUT_PROD Operator** - Full GPU support (CUDA, Vulkan, Metal)
- âœ… **Quantized Training** - Q4_0 and Q8_0 fine-tuning
- âœ… **Mixed Precision** - FP16 and FP32 training

### Platform Enhancements
- âœ… **Metal Backend** - Native Apple GPU support (iOS, macOS)
- âœ… **Vulkan Enhancements** - AMD, Intel, NVIDIA, Adreno, Mali
- âœ… **SYCL Support** - Optimized Intel GPU backend
- âœ… **ARM64** - Raspberry Pi and ARM server support

### Data Pipeline
- âœ… **ChatML Templates** - Built-in conversation formatting
- âœ… **Custom Jinja** - Flexible template system
- âœ… **JSONL Support** - HuggingFace-compatible datasets
- âœ… **Masked Loss** - Train only on assistant responses

---

## ğŸ“– Documentation

### Getting Started
- [Quick Start Guide](../README.md#quick-start)
- [Installation Instructions](../README.md#installation)
- [First Fine-Tuning Session](../README.md#finetune-lora)

### Platform Guides
- [Android Setup & Tips](./android/README.md)
- [macOS Setup & Tips](./macos/README.md)
- [iOS Setup & Tips](./ios/README.md)
- [Linux Setup & Tips](./linux/README.md)

### Advanced Topics
- [Detailed Benchmarks](../docs/BENCHMARKS.md)
- [Research Paper](../docs/paper.pdf)
- [API Reference](../docs/API.md)
- [Dataset Format](../datasets/README.md)

---

## ğŸ› Known Issues

### Mobile Platforms
- âš ï¸ Qwen3-4B causes OOM on most mobile devices (use 1.7B or smaller)
- âš ï¸ iOS may suspend background training (keep app in foreground)
- âš ï¸ Mali G715 training is slower than Adreno (functional but patience required)

### Desktop Platforms
- âš ï¸ Flash attention not yet supported on Vulkan backend
- âš ï¸ Multi-GPU training experimental (single GPU recommended)

### General
- âš ï¸ Very large batch sizes (>256) may cause OOM on some GPUs
- âš ï¸ WebGPU backend is experimental

### Workarounds
Most issues can be resolved by:
- Using smaller models or context windows
- Reducing batch size
- Using Q4_0 quantization
- Enabling checkpointing for long runs

---

## ğŸ”„ Upgrade Path

### From Previous Versions
This is the initial v1.0 release. Future versions will maintain backward compatibility.

### Model Compatibility
All models fine-tuned with v1.0 are compatible with:
- Future qvac-finetune versions
- llama.cpp inference
- Any GGUF-compatible runtime

---

## ğŸ’¡ Quick Start Examples

### Android (Termux)
```bash
export LD_LIBRARY_PATH=.
./bin/llama-finetune-lora \
  -m qwen3-0.6b-q8_0.gguf \
  -f biomedical_qa.jsonl \
  --assistant-loss-only \
  -c 128 -b 64 -ub 64 -ngl 99 -fa off \
  --num-epochs 2
```

### macOS (Apple Silicon)
```bash
./bin/llama-finetune-lora \
  -m qwen3-1.7b-q8_0.gguf \
  -f biomedical_qa.jsonl \
  --assistant-loss-only \
  -c 128 -b 128 -ub 128 -ngl 999 -fa off \
  --num-epochs 8 \
  --lora-modules "attn_q,attn_k,attn_v,attn_o,ffn_gate,ffn_up,ffn_down"
```

### Linux (Vulkan)
```bash
./bin/llama-finetune-lora \
  -m qwen3-1.7b-q8_0.gguf \
  -f biomedical_qa.jsonl \
  --assistant-loss-only \
  -c 128 -b 128 -ub 128 -ngl 999 -fa off \
  --learning-rate 1e-5 --lr-scheduler cosine \
  --checkpoint-save-steps 50 \
  --num-epochs 8
```

---

## ğŸ™ Acknowledgments

Built on the llama.cpp foundation with extensive contributions:
- GGML core engine enhancements
- Vulkan backend improvements
- Metal backend training support
- Dynamic tiling algorithm for mobile GPUs
- LoRA architecture integration

Special thanks to the llama.cpp community and all hardware vendors who provided testing devices.

---

## ğŸ“ Support & Community

- ğŸŒ [Project Website](https://github.com/akshaypn/qvac-finetune)
- ğŸ’¬ [GitHub Discussions](https://github.com/akshaypn/qvac-finetune/discussions)
- ğŸ› [Issue Tracker](https://github.com/akshaypn/qvac-finetune/issues)
- ğŸ“– [Documentation](../README.md)
- ğŸ“Š [Benchmarks](../docs/BENCHMARKS.md)
- ğŸ”¬ [Research Paper](../docs/paper.pdf)

---

## ğŸ“„ License

MIT License - See [LICENSE](../LICENSE) for details.

**Dataset Licenses:**
- Biomedical QA: MIT (PubMedQA-derived)
- Email Dataset: Internal research use

---

## ğŸ”® Roadmap

### Upcoming Features
- [ ] Multi-GPU training support
- [ ] Flash attention on Vulkan
- [ ] WebGPU backend stabilization
- [ ] Additional model architectures
- [ ] Distributed training
- [ ] Quantization-aware training improvements

### Community Requests
Vote on features in [GitHub Discussions](https://github.com/akshaypn/qvac-finetune/discussions)!

---

<div align="center">
  <p><b>Making LLM fine-tuning accessible to everyone, everywhere</b></p>
  <p>From smartphones to datacenters â€¢ No vendor lock-in â€¢ Privacy-preserving</p>
  <p>â­ Star the repo if you find it useful!</p>
</div>

